<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Applied machine learning - Group xxx: Machine Learning Model Analysis</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Applied machine learning - Group xxx
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Machine Learning Model Analysis </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>[[<em>TOC</em>]]</p>
<h1>Quick Summary</h1>
<p>Predictive time series modeling is a more or less <b>standard problem</b> in the field of statistical modeling for which <b>many different approaches</b> exist. One of the main goals during the AMI project is to evaluate some of these <b>candidate model approaches</b> and to eventually decide for one that suits the project task at hand. This underlying document thus presents a quick &amp; dirty analysis of three possible learning approaches and discusses their respective feasibility. <br  />
</p>
<h1>Extreme Learning Machines (ELMs)</h1>
<p>One very promising emerging approach to combat <b>regression and classification problems</b> is the utilization of <b>Extreme Learning Machines (ELMs)</b>. While being not so prominent in traditional ML lectures, ELMs are widely known for a <b>fairly good accuracy</b> and <b>extremely fast performance</b>. The latter follows from the fact that the respective <b>pseudo-hidden neuron parameters</b> (in the following just referred to as hidden) do not need to be tuned during learning and are thus independent of the underlying training data set. More so, the hidden neurons are rather <b>randomly generated</b> which consequently implies a <b>random initialization of its parameters</b> such as input weights, biases, centers, etc. Still, the universal approximation capability holds, for which an arbitrary model accuracy - supposing that there are enough hidden neurons - can be achieved for the regression/classification task at hand.</p>
<p>The probably most distinct property embedded in the ELM nature is the <b>non-iterative linear solution</b> for the respective output weights. This is mainly due to the independence between the input and output weights, unlike in a backpropagation scenario. This ultimately renders ELMs to be very fast compared to similar MLP and SVM solutions.</p>
<p>Most of the discussed concepts below can be re-read in the following articles:</p><ul>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7140733">High-Performance Extreme Learning Machines: A Complete Toolbox for Big Data Applications</a></li>
<li><a href="http://www.di.unito.it/~cancelli/retineu11_12/ELM-NC-2006.pdf">Extreme learning machine: Theory and applications</a></li>
<li><a href="https://csce.ucmss.com/cr/books/2018/LFS/CSREA2018/ICA4128.pdf">tfelm: a TensorFlow Toolbox for the Investigation of ELMs and MLPs Performance</a></li>
</ul>
<h2>ELM Model</h2>
<p>ELMs are <b>fast training methods</b> for <b>single layer feed-forward neural networks (SLFN)</b>. Once again, this is because input weights W and biases b are randomly set and never adjusted. Consequently, the respective output weights β are independent. Furthermore, the <b>randomness of the input layer weights</b> improves the generalization property w.r.t. the solution of a linear output layer. The so induced orthogonality leads to almost orthogonal and thus <b>weakly correlated</b> hidden layer features.</p>
<p>In general, we can define an ELM model as follows. Consider a set of N distinct training samples (x_i, t_i) where i ranges between 1 and N. The SLFN output equation with L hidden neurons can then be denoted as</p>
<p><img src="/documentation/Machine Learning Models/images/SLFN_output.png" alt="SLFN Output Equation" width="200" class="inline"/></p>
<p>with φ being the activation function (usually a sigmoid), w_i the input weights, b_i the biases and β_i the respective output weights. Consequently, the relation between the network inputs x_i, the target outputs t_i and the estimated outputs y_i is given by</p>
<p><img src="/documentation/Machine Learning Models/images/estimated_output.png" alt="Estimated Model Output" width="300" class="inline"/></p>
<p>where ε denotes the noise comprised of random noise and certain dependencies on hidden variables excluded from the inputs x_i. This process can be re-examined in below figure.</p>
<p><img src="/documentation/Machine Learning Models/images/SLFN_process.png" alt="ELM SLFN Process" width="500" class="inline"/></p>
<h2>Computation</h2>
<p>Before discussing the simple computation technique behind ELMs, it is reasonable to first discuss the processes behind the respective hidden neurons as well as a compact matrix notation.</p>
<h3>Pseudo-Hidden Neurons</h3>
<p>In general, the hidden neurons <b>transform</b> the underlying input data into a different representation. This is usually done in two steps: 1) The data is projected into the hidden layer using the input layer weights and biases. 2) The projected data is transformed using a non-linear transformation function.</p>
<p>In particular, using above <b>non-linear transformation</b>, the learning capabilities of the ELM can be greatly <b>increased</b>. After transformation, the data in the hidden layers h_I can be used to find the output layer weights. Another <b>practical advantage</b> is that the respective transformation functions are not constrained by type, that is they can be selected to be very different and even non-existent. Furthermore, since the neurons are linear, they consequently adapt and learn linear dependencies between data features and targets which happens directly without any nonlinear approximation at all. With that in mind, it becomes clear that the number of neurons <b>must equal</b> the number of data features.</p>
<p>Note however, that other types of neurons have also found application in ELMs such as <b>RBF neurons</b> with nonlinear projection functions. These can be used to compute predictions based on similar training data samples in order to solve tasks with some more complex dependencies between data features and targets.</p>
<h3>Compact Matrix Notation</h3>
<p>ELMs exhibit a <b>closed form solution</b> in which the hidden neurons are comprised in a matrix H. The network structure itself though is not noticable in practice meaning that there is only a <b>single matrix</b> that describes the projection between two - usually linear - spaces. The projections for the input (X⋅W) and the output (H⋅β) are connected through a nonlinear transformation as follows.</p>
<p><img src="/documentation/Machine Learning Models/images/nonlinear_trafo.png" alt="Nonlinear Transformation" width="100" class="inline"/></p>
<p>The number of hidden neurons thus consequently <b>regulates</b> the size of the matrices W, H and β. However, the network neurons are never treated separately. With different types of hidden neurons, the first projection and transformation are performed <b>independently</b> for each type of neuron. Then the resulting sub-matrices H_1 are concatenated along the second dimension. For two types of hidden neurons it follows that</p>
<p><img src="/documentation/Machine Learning Models/images/H_1.png" alt="H Notation" width="300" class="inline"/></p>
<p>where linear neurons are added by simply copying the inputs into the hidden layer outpus</p>
<p><img src="/documentation/Machine Learning Models/images/H_2.png" alt="Extended H Notation" width="300" class="inline"/></p>
<h3>Solution Computation</h3>
<p>In general, ELM problems are usually <b>over-determined (N&gt;L)</b> with the number of training data samples being much larger than the number of selected hidden neurons. In all other cases (N&lt;=L), regularization should be used in order to obtain a better generalization performance.</p>
<p>Nevertheless, a unique solution can be found using the pseudoinverse:</p>
<p><img src="/documentation/Machine Learning Models/images/H_solution.png" alt="Solution Computation" width="140" class="inline"/></p>
<h2>Conclusion</h2>
<p>In order to summarize, ELMs have very promising and efficient properties. They have been proven to be very useful for regression tasks as needed in our project. Nevertheless, there have been some reports on negative effects such as</p><ul>
<li>Bad initial randomization</li>
<li>Speedy performance but low accuracy</li>
<li>Need of regularization options</li>
</ul>
<p>In particular, it has to be pointed out that ELMs only operate on <b>one hidden layer</b> in contrast to general DL approaches. In order to achieve a very high accuracy and approximation, this might not be necessarily the best performing choice considering the amount of COVID-19 data that we have gathered throughout the collection process.</p>
<p>Nevertheless, there are two very high-performance <b>Matlab and Python implementations</b> in form of <b>ready-to-use toolboxes</b> discussed in above articles. They promise automatic model structure selection as well as the application of regularization techniques. Furthermore, many approaches using self-written Python code have emerged online.</p>
<p>However, the main argument that seems to disqualify ELMs - at least from our user point of view - is that <b>only one group member</b> has even had any experience at all using them. Furthermore, even though the concepts of ELMs seem promising and effective, most of the ML experience was simply gained in Python using <b>TensorFlow and PyTorch</b>. The goal thus remains to find a time series modeling approach which exploits the vast availability of pre-built functions as found in these aforementioned frameworks. <br  />
</p>
<h1>Gaussian Processes (GPs)</h1>
<p>Another very promising approach which solves <b>regressional time series problems</b> is the use of <b>Gaussian Processes (GPs)</b> for which a handful of implementations is given in the <b>scikit ML library</b>. In particular, there have already been attempts to model and forecast CO2 emissions using GPs, as done <a href="https://stats.stackexchange.com/questions/377999/why-are-gaussian-processes-valid-statistical-models-for-time-series-forecasting">here</a>. This not only <b>motivates</b> to further investigate GPs for our project but also demonstrates a successful application, ultimately rendering this approach as <b>highly plausible</b> to achieve our specified project target.</p>
<h2>GP Model</h2>
<p>GPs are a very generic class of <b>supervised learning methods</b> which are designed to not only solve <b>regression but also probabilitsic classification problems</b>. In general, a GP is a <b>stochastic process</b> and thus a collection of random variables, e.g. in the time or space domain. Note that every such finite collection of random variables has a <b>multivariate normal distribution</b>, that is every finite linear combination of these random variables is strictly <b>normally distributed</b>. As such, every GP can be compactly described by the <b>joint distribution</b> of all those random variables and is thus strictly specified by its <b>mean and covariance functions</b>.</p>
<p>A GP can be described as a functional mapping of random variables x_i</p>
<p><img src="/documentation/Machine Learning Models/images/GP_mapping.png" alt="GP Mapping" width="100" class="inline"/></p>
<p>with mean function m(x)</p>
<p><img src="/documentation/Machine Learning Models/images/GP_mean.png" alt="GP Mean" width="100" class="inline"/></p>
<p>and covariance function k(x, x')</p>
<p><img src="/documentation/Machine Learning Models/images/GP_cov.png" alt="GP Covariance" width="240" class="inline"/></p>
<p>Most ML algorithms that make use of GPs often apply <b>lazy learning approaches</b> in order to measure the <b>similarity</b> between the respective evaluation points. To this, the so-called <b>kernel function</b> is examined which aids to predict the value for a future, e.g. time series point. The so obtained prediction - in form of a <b>distribution</b> - not just provides an estimation but also contains some <b>uncertainty information</b> which is embedded in the <b>one-dimensional Gaussian distribution</b>. The same holds for multidimensional predictions where the GP is multivariate and for which the respective multivariate Gaussian distributions are the corresponding marginal distributions at the current evaluation point.</p>
<h2>Pros and Cons</h2>
<p>The advantages of GP models can mainly be summarized as follows:</p><ul>
<li>The model prediction interpolates between the observations for regular kernels.</li>
<li>The prediction is probabilistic and allows for an analysis of confidence intervals which in turn aids to decide whether refitting is necessary. The latter one can thus be solved in an online fashion.</li>
<li>There is a certain versatitlity due to the possibility to choose differently specified kernels.</li>
</ul>
<p>However, there are also some severe disadvantages of GP models:</p><ul>
<li>Non-sparsity: The models use the whole sample space and all feature information in order to perform a prediction.</li>
<li>Low efficiency in high-dimensional (&gt;12) spaces.</li>
</ul>
<h2>Conclusion</h2>
<p>While GP models bring many different advantages and are also quite broady represented in the desired frameworks to be used during the project, once again <b>only few group members</b> have actively dealt with both the <b>theory and practical implementation</b> of such models. Thus, diving deeper in the more complex stochastic modeling theory will certainly <b>take up more time</b> than we initially desired to give for the ML core development. As such, we decide that more time should be spent <b>analyzing, optimizing and troubleshooting</b> the model output which is frankly the more <b>exhausting</b> part for a successful model application. Therefore, we live by the notion that the <b>more simple the model the better</b>. Since most of the group members have gained considerable experience in DL modeling scenarios, these are to choose and GP models will thus not be further considered. <br  />
</p>
<h1>LSTM RNN</h1>
<p>What all above ML approaches have led to is a more or less pre-determined decision for a <b>DL solution</b>. In particular, there are many time-series modeling approaches using <b>recurrent neural networks (RNNs)</b>. However, while these seem to achieve some <b>very high accuracies</b>, they also suffer from severe drawbacks which might also affect our project scope due to the vast amount of data that we have gathered. To be precise, one particularly dangerous disadvantage of conventional RNNs is their <b>short-term memory capacity</b>. In order to combat this drawback, <b>long short-term memory (LSTM) RNNs</b> have been introduced that incorporate a significantly <b>greater (longer) memory capacity</b>.</p>
<p>Unlike feed-forward neural networks, LSTMs have <b>feedback connections</b> and are not only able to process <b>single data points</b> but also <b>entire sequences</b> - a character trait especially needed for our project implementation! A huge proportion of LSTM associated model applications so far have been based on time series data. In particular, some models - as presented <a href="https://www.curiousily.com/posts/time-series-forecasting-with-lstm-for-daily-coronavirus-cases/">here</a> - have been constructed that actively address the COVID-19 pandemic and use time-series data for accurate predictions. This once again <b>motivates</b> to further discuss LSTM RNNs as a valid approach for our project realization.</p>
<h2>LSTM core idea</h2>
<p>In general, RNNs can be represented in a <b>chain-like form of repeating modules</b> which incorporate loops and interconnections. LSTMs in particular introduce a repeating module which has a more advanced structure containing <b>several neural network layers</b>. The general chain layout for the repeating module of such LSTMs is depicted below.</p>
<p><img src="/documentation/Machine Learning Models/images/LSTM_layout.png" alt="LSTM Repeating Module Layout" width="300" class="inline"/></p>
<p><img src="/documentation/Machine Learning Models/images/LSTM_tools.png" alt="LSTM Tools" width="300" class="inline"/></p>
<p>The core of LSTM based RNNs is the <b>cell state C</b> which is defined by the horizontal line on the top of the respective module:</p>
<p><img src="/documentation/Machine Learning Models/images/LSTM_cellState.png" alt="LSTM Cell State" width="300" class="inline"/></p>
<p>In particular, the cell state interacts <b>linearly</b> with other elements throughout its way. The main feature of LSTMs is the ability to <b>add and remove</b> certain information w.r.t. the cell state, that is information can be <b>sequentially</b> added to and removed from it. This mechanism is regulated by so-called <b>gates</b> which will be examined in more detail in the following subsection.</p>
<h2>LSTM Information Gates</h2>
<p>The gate structures are usually composed of <b>sigmoid neural net layers</b> and <b>pointwise multiplication operators</b>. Depending on their function, we can classify them as forget, input and output gates.</p>
<h3>Forget Gate</h3>
<p>The forget gate more or less describes the first decision that the network has to make: What information has to be <b>thrown away</b> from the cell state. It thus decides whether information will be <b>deleted or not</b>. In order to do so, the gate considers the <b>previous h_(t-1) and current x_t value</b>, passes both through the sigmoid layer and computes a value <b>f_t between 0 and 1</b> for the current cell state C_t which describes the <b>memory degree</b> (0 = forget entirely, 1 = remember entirely).</p>
<p><img src="/documentation/Machine Learning Models/images/LSTM_forgetGate.png" alt="LSTM Forget Gate" width="400" class="inline"/></p>
<h3>Input Gate</h3>
<p>The next consequent step for the LSTM is decide what <b>new information is going to be added</b> to the current cell state. This is performed by the input gate which consists of two parts:</p><ul>
<li>First, a sigmoid layer - the so-called input gate layer - decides which former values will be updated.</li>
<li>Second, a tanh layer creates a dedicated vector of new candidate values that could be added to the cell state.</li>
</ul>
<p><img src="/documentation/Machine Learning Models/images/LSTM_inputGate_1.png" alt="LSTM Input Gate (1)" width="400" class="inline"/></p>
<p>Furthermore, the now old cell state C_(t-1) has to be updated to the new cell state C_t by multiplying the old state with f_t - forgetting the information we decided to forget in the forget gate - and by adding above multiplication of i_t with the candidate vector.</p>
<p><img src="/documentation/Machine Learning Models/images/LSTM_inputGate_2.png" alt="LSTM Input Gate (2)" width="400" class="inline"/></p>
<h3>Output Gate</h3>
<p>Last but not least, the LSTM has to decide <b>what to output</b>. This is highly dependent on the cell state and is essentially going to be a <b>filtered version</b> of it performed by the output gate:</p><ul>
<li>First, a sigmoid layer is run for h_(t-1) and x_t analogously to above input gate.</li>
<li>Second, the current cell state is pushed through a tanh layer which confines a value space between -1 and 1.</li>
<li>Third, we multiply each outcome and obtain the consecutive value for h_t which reflects all our previous decisions.</li>
</ul>
<p><img src="/documentation/Machine Learning Models/images/LSTM_outputGate.png" alt="LSTM Output Gate" width="400" class="inline"/></p>
<h2>Conclusion</h2>
<p>To summarize, LSTM RNNs have a <b>very high and most importantly proven performance</b> w.r.t. time series modeling. In particular, there are also <b>many guides and tutorials</b> on how to realize and troubleshoot (optimize) LSTM RNNs using Python. Due to their <b>DL character</b>, they are furthermore very <b>familiar</b> to the group members as multiple layers can and should be constructed. In particular, there is also <b>no higher effort in understanding</b> the advanced LSTM architecture and its many variants as they can be <b>astractly considered</b> as "just elements in a RNN". As such, LSTM RNNs are currently the most promising solution to the time series model approach that we intend for our project scope.</p>
<p>A sample realization of LSTM RNNs using Keras in Python is demonstrated <a href="https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/">in this article</a>, a time series weather forecasting guide using LSTMs in Python has been shown <a href="https://www.tensorflow.org/tutorials/structured_data/time_series">in this approach</a> and above COVID-19 forecast using LSTMs can once again be found <a href="https://www.curiousily.com/posts/time-series-forecasting-with-lstm-for-daily-coronavirus-cases/">here</a>.</p>
<h1>Decision</h1>
<p>After having reviewed above candidate ML models that <b>certainly would all suffice</b> to achieve the project task at hand, a final decision is hard to make. <b>LSTM-based RNNs</b> in particular seem to be a very powerful solution to the problem. Furthermore, the <b>matureness of LSTMs</b> compared to Extreme Learning and GP models as well as their <b>familiarity to every group member</b> - as they can be seen as an extension to RNNs - are two of the main driving factors fueling the decision for it. However, one major drawback that has emerged throughout the <b>data collection phase</b> is that not all of our data sets are of a particularly high quality due to <b>noise</b> and other factors. Thus, we believe that performing predictions using LSTM-based RNNs <b>will not exploit above high-feature advantages</b> nor will the predictions be of a particularly high accuracy due to the <b>low-quality data</b> in some cases. Furthermore, we believe that above candidate approaches need to be compared to more <b>traditional regression approaches</b> such as linear regression and decision trees which are well-known to the group members. In particular, necessary obstacles such as <b>troubleshooting, hyperparameter optimizing and model verification</b> are well understood for these regression model classes. In conclusion, very powerful model architectures have been discussed and examined. However, the preliminary performance assessment will determine whether complex approaches <b>outperform</b> standard regression models. A <b>final decision</b> on the model architecture can certainly only be made <b>after the assessment</b>. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
